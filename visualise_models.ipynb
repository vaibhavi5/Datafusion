{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def forward(self, x):\\n        layer_names = []\\n        print(x.shape)\\n        #x = self.features(x)\\n        layer = self.features[0]\\n        hold = layer(x)\\n        hold = hold.detach().cpu().numpy()\\n        np.save(\"layer_1\",hold[0])\\n        for i, layer in enumerate(self.features):\\n            x = layer(x)\\n            layer_name = f\"layer_{i}\"\\n            print(f\"{layer_name}: {x.size()}\")\\n            layer_names.append(layer_name)\\n        x = x.view(x.size(0), -1)\\n        print(\"final:\", x.shape)\\n        x = self.classifier(x)\\n        return [x]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from math import *\n",
    "import numpy as np\n",
    "import torch\n",
    "num_channels = 1 #Depending if its multimodal or just fMRI \n",
    "\n",
    "class AlexNet3D_Dropout(nn.Module): #AlexNet3D_Deeper_Dropout\n",
    "    def __init__(self, num_classes=2):  #This is dependent on three way = 3, two way = 2 and regression = 1\n",
    "        super(AlexNet3D_Dropout, self).__init__() #ModuleList\n",
    "        \n",
    "        self.features = nn.Sequential(        \n",
    "            nn.Conv3d(num_channels, 64*num_channels, kernel_size=5,\n",
    "                      stride=2, padding=0, groups=num_channels), #kernel 5 and padding 0 for uni, kernel 3 and padding 1 for multi\n",
    "            nn.BatchNorm3d(64*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=3),\n",
    "\n",
    "            nn.Conv3d(64*num_channels, 128*num_channels, kernel_size=3,\n",
    "                      stride=1, padding=0, groups=num_channels),\n",
    "            nn.BatchNorm3d(128*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=3),\n",
    "\n",
    "            nn.Conv3d(128*num_channels, 192*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(192*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(192*num_channels, 384*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(384*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(384*num_channels, 256*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(256*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(256*num_channels, 256*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(256*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool3d(kernel_size=3, stride=3)\n",
    "            nn.AdaptiveAvgPool3d([1,1,1]),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Dropout(),\n",
    "                                        nn.Linear(256*num_channels, 64),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(),\n",
    "                                        nn.Linear(64, num_classes),\n",
    "                                        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x  \n",
    "\n",
    "'''    def forward(self, x):\n",
    "        xp = self.features(x)\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            print (i, x.size())\n",
    "        x = xp.view(xp.size(0), -1)\n",
    "        #print(\"final, \", x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return [x, xp]'''\n",
    "\n",
    "'''    def forward(self, x):\n",
    "        layer_names = []\n",
    "        print(x.shape)\n",
    "        #x = self.features(x)\n",
    "        layer = self.features[0]\n",
    "        hold = layer(x)\n",
    "        hold = hold.detach().cpu().numpy()\n",
    "        np.save(\"layer_1\",hold[0])\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            layer_name = f\"layer_{i}\"\n",
    "            print(f\"{layer_name}: {x.size()}\")\n",
    "            layer_names.append(layer_name)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(\"final:\", x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return [x]'''\n",
    "\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Sequential\n",
      "Input shape: (1, 53, 63, 52)\n",
      "Layer: Sequential\n",
      "Input shape: (1, 53, 63, 52)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 64, 25, 30, 24]           8,064\n",
      "       BatchNorm3d-2       [-1, 64, 25, 30, 24]             128\n",
      "              ReLU-3       [-1, 64, 25, 30, 24]               0\n",
      "         MaxPool3d-4         [-1, 64, 8, 10, 8]               0\n",
      "            Conv3d-5         [-1, 128, 6, 8, 6]         221,312\n",
      "       BatchNorm3d-6         [-1, 128, 6, 8, 6]             256\n",
      "              ReLU-7         [-1, 128, 6, 8, 6]               0\n",
      "         MaxPool3d-8         [-1, 128, 2, 2, 2]               0\n",
      "            Conv3d-9         [-1, 192, 2, 2, 2]         663,744\n",
      "      BatchNorm3d-10         [-1, 192, 2, 2, 2]             384\n",
      "             ReLU-11         [-1, 192, 2, 2, 2]               0\n",
      "           Conv3d-12         [-1, 384, 2, 2, 2]       1,991,040\n",
      "      BatchNorm3d-13         [-1, 384, 2, 2, 2]             768\n",
      "             ReLU-14         [-1, 384, 2, 2, 2]               0\n",
      "           Conv3d-15         [-1, 256, 2, 2, 2]       2,654,464\n",
      "      BatchNorm3d-16         [-1, 256, 2, 2, 2]             512\n",
      "             ReLU-17         [-1, 256, 2, 2, 2]               0\n",
      "           Conv3d-18         [-1, 256, 2, 2, 2]       1,769,728\n",
      "      BatchNorm3d-19         [-1, 256, 2, 2, 2]             512\n",
      "             ReLU-20         [-1, 256, 2, 2, 2]               0\n",
      "AdaptiveAvgPool3d-21         [-1, 256, 1, 1, 1]               0\n",
      "          Dropout-22                  [-1, 256]               0\n",
      "           Linear-23                   [-1, 64]          16,448\n",
      "             ReLU-24                   [-1, 64]               0\n",
      "          Dropout-25                   [-1, 64]               0\n",
      "           Linear-26                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 7,327,490\n",
      "Trainable params: 7,327,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.66\n",
      "Forward/backward pass size (MB): 27.74\n",
      "Params size (MB): 27.95\n",
      "Estimated Total Size (MB): 56.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define a function to print the input shape\n",
    "def print_input_shape(layer, input_size):\n",
    "    print(f\"Layer: {layer.__class__.__name__}\")\n",
    "    print(f\"Input shape: {input_size}\")\n",
    "    return input_size\n",
    "\n",
    "# Initialize your model\n",
    "model = AlexNet3D_Dropout()\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (num_channels, 53, 63, 52)\n",
    "\n",
    "# Print the input shape for each layer in the model\n",
    "input_size = input_shape\n",
    "for layer in model.children():\n",
    "    input_size = print_input_shape(layer, input_size)\n",
    "\n",
    "# Create a summary of the model\n",
    "summary(model, input_shape, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 64, 25, 30, 24]           8,064\n",
      "       BatchNorm3d-2       [-1, 64, 25, 30, 24]             128\n",
      "              ReLU-3       [-1, 64, 25, 30, 24]               0\n",
      "         MaxPool3d-4         [-1, 64, 8, 10, 8]               0\n",
      "            Conv3d-5         [-1, 128, 6, 8, 6]         221,312\n",
      "       BatchNorm3d-6         [-1, 128, 6, 8, 6]             256\n",
      "              ReLU-7         [-1, 128, 6, 8, 6]               0\n",
      "         MaxPool3d-8         [-1, 128, 2, 2, 2]               0\n",
      "            Conv3d-9         [-1, 192, 2, 2, 2]         663,744\n",
      "      BatchNorm3d-10         [-1, 192, 2, 2, 2]             384\n",
      "             ReLU-11         [-1, 192, 2, 2, 2]               0\n",
      "           Conv3d-12         [-1, 384, 2, 2, 2]       1,991,040\n",
      "      BatchNorm3d-13         [-1, 384, 2, 2, 2]             768\n",
      "             ReLU-14         [-1, 384, 2, 2, 2]               0\n",
      "           Conv3d-15         [-1, 256, 2, 2, 2]       2,654,464\n",
      "      BatchNorm3d-16         [-1, 256, 2, 2, 2]             512\n",
      "             ReLU-17         [-1, 256, 2, 2, 2]               0\n",
      "           Conv3d-18         [-1, 256, 2, 2, 2]       1,769,728\n",
      "      BatchNorm3d-19         [-1, 256, 2, 2, 2]             512\n",
      "             ReLU-20         [-1, 256, 2, 2, 2]               0\n",
      "AdaptiveAvgPool3d-21         [-1, 256, 1, 1, 1]               0\n",
      "          Dropout-22                  [-1, 256]               0\n",
      "           Linear-23                   [-1, 64]          16,448\n",
      "             ReLU-24                   [-1, 64]               0\n",
      "          Dropout-25                   [-1, 64]               0\n",
      "           Linear-26                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 7,327,490\n",
      "Trainable params: 7,327,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.66\n",
      "Forward/backward pass size (MB): 27.74\n",
      "Params size (MB): 27.95\n",
      "Estimated Total Size (MB): 56.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "#from your_model_file import AlexNet3D_Dropout  # Import your model class\n",
    "\n",
    "# Initialize your model\n",
    "model = AlexNet3D_Dropout()\n",
    "\n",
    "# Create a summary of the model, specifying the input shape\n",
    "summary(model, (num_channels, 53, 63, 52), device=\"cpu\")\n",
    "\n",
    "# Replace the values of input_depth, input_height, and input_width with your data dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "#from your_model_file import AlexNet3D_Dropout  # Import your model class\n",
    "\n",
    "# Initialize your model\n",
    "model = AlexNet3D_Dropout()\n",
    "\n",
    "# Create a random input tensor to trace the model\n",
    "sample_input = torch.randn((1, 1, 53, 63, 52))\n",
    "\n",
    "# Trace the model using torchviz\n",
    "model_trace = torch.jit.trace(model, sample_input)\n",
    "\n",
    "# Create a visualization of the model\n",
    "dot = torchviz.make_dot(model_trace(sample_input))\n",
    "dot.format = 'png'\n",
    "dot.render(\"model_visualization\")  # Save the visualization as a PNG file\n",
    "\n",
    "# Replace the values of num_channels, input_depth, input_height, and input_width with your data dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users2/vitkyal/softwares/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from math import *\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "num_channels = 1 #Depending if its multimodal or just fMRI \n",
    "\n",
    "class AlexNet3D_Dropout(nn.Module): #AlexNet3D_Deeper_Dropout\n",
    "    def __init__(self, num_classes=2):  #This is dependent on three way = 3, two way = 2 and regression = 1\n",
    "        super(AlexNet3D_Dropout, self).__init__() #ModuleList\n",
    "        \n",
    "        self.features = nn.Sequential(        \n",
    "            nn.Conv3d(num_channels, 64*num_channels, kernel_size=5,\n",
    "                      stride=2, padding=0, groups=num_channels), #kernel 5 and padding 0 for uni, kernel 3 and padding 1 for multi\n",
    "            nn.BatchNorm3d(64*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=3),\n",
    "\n",
    "            nn.Conv3d(64*num_channels, 128*num_channels, kernel_size=3,\n",
    "                      stride=1, padding=0, groups=num_channels),\n",
    "            nn.BatchNorm3d(128*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=3),\n",
    "\n",
    "            nn.Conv3d(128*num_channels, 192*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(192*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(192*num_channels, 384*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(384*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(384*num_channels, 256*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(256*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(256*num_channels, 256*num_channels,\n",
    "                      kernel_size=3, padding=1, groups=num_channels),\n",
    "            nn.BatchNorm3d(256*num_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool3d(kernel_size=3, stride=3)\n",
    "            nn.AdaptiveAvgPool3d([1,1,1]),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Dropout(),\n",
    "                                        nn.Linear(256*num_channels, 64),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(),\n",
    "                                        nn.Linear(64, num_classes),\n",
    "                                        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'distutils' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/users2/vitkyal/projects/SMLvsDL/reprex/visualise_models.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Barctrdgndev101-vitkyal/data/users2/vitkyal/projects/SMLvsDL/reprex/visualise_models.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m sample_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m1\u001b[39m, num_channels, \u001b[39m53\u001b[39m, \u001b[39m63\u001b[39m, \u001b[39m52\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Barctrdgndev101-vitkyal/data/users2/vitkyal/projects/SMLvsDL/reprex/visualise_models.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Add the model graph to TensorBoard\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Barctrdgndev101-vitkyal/data/users2/vitkyal/projects/SMLvsDL/reprex/visualise_models.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m writer\u001b[39m.\u001b[39;49madd_graph(model, sample_input)\n",
      "File \u001b[0;32m/data/users2/vitkyal/softwares/miniconda3/envs/pytorch/lib/python3.9/site-packages/tensorboardX/writer.py:939\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_graph\u001b[39m(\n\u001b[1;32m    923\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m         model,\n\u001b[1;32m    925\u001b[0m         input_to_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    926\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    927\u001b[0m         use_strict_trace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    928\u001b[0m     \u001b[39m\"\"\"Add graph data to summary. The graph is actually processed by `torch.utils.tensorboard.add_graph()`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \n\u001b[1;32m    930\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[39m            record your mutable container types (list, dict)\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 939\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pytorch_graph\u001b[39;00m \u001b[39mimport\u001b[39;00m graph\n\u001b[1;32m    940\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_graph(graph(model, input_to_model, verbose, use_strict_trace\u001b[39m=\u001b[39muse_strict_trace))\n",
      "File \u001b[0;32m/data/users2/vitkyal/softwares/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msetuptools\u001b[39;00m \u001b[39mimport\u001b[39;00m distutils\n\u001b[0;32m----> 4\u001b[0m LooseVersion \u001b[39m=\u001b[39m distutils\u001b[39m.\u001b[39;49mversion\u001b[39m.\u001b[39mLooseVersion\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensorboard, \u001b[39m'\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m LooseVersion(tensorboard\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(\u001b[39m'\u001b[39m\u001b[39m1.15\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mTensorBoard logging requires TensorBoard version 1.15 or above\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'distutils' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "\n",
    "# Initialize the SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Import your model (e.g., AlexNet3D_Dropout) and create an instance\n",
    "model = AlexNet3D_Dropout()\n",
    "\n",
    "# Create a dummy input (replace this with your actual input)\n",
    "sample_input = torch.randn((1, num_channels, 53, 63, 52))\n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "writer.add_graph(model, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e81493da29025d983c6d04e3b9bf8a9f5c389919a754d23fc827784e9aaef791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
